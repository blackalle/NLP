{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2: PRÁCTICA DE CLASIFICACIÓN DE TEXTOS\n",
    "## Borja Lacalle Álvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librerias para el desarrollo de la actividad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leer dataset spam.csv\n",
    "spam = pd.read_csv('spam.csv')\n",
    "spam.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ver porcentaje de spam y no spam\n",
    "spam['label'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizar el texto quitando signos de puntuacion y stopwords, entre otras cosas\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wpt = WordPunctTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_document(doc):\n",
    "    doc = doc.replace(\"!\", \"\").replace(\"¡\", \"\").replace(\",\", \"\").replace(\".\", \"\").replace(\";\", \"\")\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazy available bugis n great world la e buffet cine got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " \"free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry question ( std txt rate ) & c ' apply 08452810075over18 '\",\n",
       " 'u dun say early hor u c already say',\n",
       " \"nah ' think goes usf lives around though\",\n",
       " \"freemsg hey darling ' 3 week ' word back ' like fun still ? tb ok xxx std chgs send å £ 150 rcv\",\n",
       " 'even brother like speak treat like aids patent',\n",
       " \"per request ' melle melle ( oru minnaminunginte nurungu vettam )' set callertune callers press * 9 copy friends callertune\",\n",
       " 'winner valued network customer selected receivea å £ 900 prize reward claim call 09061701461 claim code kl341 valid 12 hours',\n",
       " 'mobile 11 months ? u r entitled update latest colour mobiles camera free call mobile update co free 08002986030',\n",
       " \"' gonna home soon ' want talk stuff anymore tonight k ? ' cried enough today\",\n",
       " 'six chances win cash 100 20000 pounds txt > csh11 send 87575 cost 150p / day 6days 16 + tsandcs apply reply hl 4 info',\n",
       " 'urgent 1 week free membership å £ 100000 prize jackpot txt word : claim : 81010 & c wwwdbuknet lccltd pobox 4403ldnw1a7rw18',\n",
       " \"' searching right words thank breather promise wont take help granted fulfil promise wonderful blessing times\",\n",
       " 'date sunday',\n",
       " 'xxxmobilemovieclub : use credit click wap link next txt message click >> http :// wap xxxmobilemovieclubcom ? n = qjkgighjjgcbl',\n",
       " \"oh ki ' watching :)\",\n",
       " 'eh u remember 2 spell name yes v naughty make v wet',\n",
       " 'fine thatåõs way u feel thatåõs way gota b',\n",
       " 'england v macedonia - dont miss goals / team news txt ur national team 87077 eg england 87077 try : wales scotland 4txt / ì¼120 poboxox36504w45wq 16 +']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_spam=[]\n",
    "\n",
    "for document in spam['text']:\n",
    "    norm_spam.append(normalize_document(document))\n",
    "\n",
    "#ver solo los primeros 20 documentos normalizados\n",
    "norm_spam[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9042)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convertir norm_spam en una matriz tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(norm_spam)\n",
    "\n",
    "tfidf_array = tfidf.toarray()\n",
    "tfidf_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividir el dataset en train (80%) y test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, spam['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clasificador bayesiano ingenuo (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entenar modelo bayesiano ingenuo\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'spam'], dtype='<U4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predecir con el modelo entrenado\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "y_pred_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[965,   0],\n",
       "       [ 37, 113]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluar el modelo con la matriz de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_conf = confusion_matrix(y_test, y_pred_nb)\n",
    "nb_conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura típica de una matriz de confusión binaria es la siguiente:\n",
    "\n",
    "[[Verdaderos Negativos (TN), Falsos Positivos (FP)]  \n",
    " [Falsos Negativos (FN), Verdaderos Positivos (TP)]]\n",
    "\n",
    "Por lo tanto, estos valores tienen el siguiente significado:\n",
    "\n",
    "TN (True Negatives): La cantidad de mensajes clasificados correctamente como \"ham\" (no spam).  \n",
    "FP (False Positives): La cantidad de mensajes que fueron clasificados incorrectamente como \"spam\" cuando en realidad son \"ham\".  \n",
    "FN (False Negatives): La cantidad de mensajes que fueron clasificados incorrectamente como \"ham\" cuando en realidad son \"spam\".  \n",
    "TP (True Positives): La cantidad de mensajes clasificados correctamente como \"spam\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9668161434977578"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar el modelo con accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_acc = accuracy_score(y_test, y_pred_nb)\n",
    "nb_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo acierta el 96.68% de las predicciones en el conjunto de evaluación. Es una medida de qué tan bien el modelo está clasificando correctamente los mensajes como spam o no spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Máquina SVM (SUPPORT VECTOR MACHINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenar el modelo de svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predecir con el modelo entrenado\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "y_pred_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[963,   2],\n",
       "       [ 29, 121]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar el modelo con la matriz de confusion\n",
    "svm_conf = confusion_matrix(y_test, y_pred_svm)\n",
    "svm_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9721973094170404"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar el modelo con accuracy\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "svm_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árboles de Decisión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenar el modelo de arbol de decision\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predecir con el modelo entrenado\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "y_pred_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[951,  14],\n",
       "       [ 20, 130]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar el modelo con la matriz de confusion\n",
    "dt_conf = confusion_matrix(y_test, y_pred_dt)\n",
    "dt_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9695067264573991"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluar el modelo con accuracy\n",
    "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
    "dt_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "  Accuracy: 96.68%\n",
      "  Confusion Matrix:\n",
      "[[965   0]\n",
      " [ 37 113]]\n",
      "\n",
      "Model: SVM\n",
      "  Accuracy: 97.22%\n",
      "  Confusion Matrix:\n",
      "[[963   2]\n",
      " [ 29 121]]\n",
      "\n",
      "Model: Decision Tree\n",
      "  Accuracy: 96.95%\n",
      "  Confusion Matrix:\n",
      "[[951  14]\n",
      " [ 20 130]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store the evaluation metrics in a dictionary for easy comparison\n",
    "model_metrics = {\n",
    "    \"Naive Bayes\": {\n",
    "        \"Accuracy\": nb_acc,\n",
    "        \"Confusion Matrix\": nb_conf\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"Accuracy\": svm_acc,\n",
    "        \"Confusion Matrix\": svm_conf\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Accuracy\": dt_acc,\n",
    "        \"Confusion Matrix\": dt_conf\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to print the comparison\n",
    "def print_model_comparison_percentage(metrics):\n",
    "    for model, data in metrics.items():\n",
    "        accuracy_percentage = data['Accuracy'] * 100\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Accuracy: {accuracy_percentage:.2f}%\")\n",
    "        print(f\"  Confusion Matrix:\\n{data['Confusion Matrix']}\\n\")\n",
    "\n",
    "# Print the comparison with updated format\n",
    "print_model_comparison_percentage(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Tiene influencia en el resultado final el número máximo de features a utilizar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1000 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes (1000 features)\n",
      "  Accuracy: 97.94%\n",
      "  Confusion Matrix:\n",
      "[[962   3]\n",
      " [ 20 130]]\n",
      "\n",
      "Model: SVM (1000 features)\n",
      "  Accuracy: 97.94%\n",
      "  Confusion Matrix:\n",
      "[[963   2]\n",
      " [ 21 129]]\n",
      "\n",
      "Model: Decision Tree (1000 features)\n",
      "  Accuracy: 96.05%\n",
      "  Confusion Matrix:\n",
      "[[946  19]\n",
      " [ 25 125]]\n",
      "\n",
      "(5572, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the TF-IDF Vectorizer with max_features set to 1000\n",
    "tfidf_vectorizer_1000 = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_1000 = tfidf_vectorizer_1000.fit_transform(norm_spam)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_1000, X_test_1000, y_train_1000, y_test_1000 = train_test_split(tfidf_1000, spam['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Reinitialize and train the Naive Bayes classifier\n",
    "nb_classifier_1000 = MultinomialNB()\n",
    "nb_classifier_1000.fit(X_train_1000, y_train_1000)\n",
    "\n",
    "# Reinitialize and train the SVM classifier\n",
    "svm_classifier_1000 = SVC()\n",
    "svm_classifier_1000.fit(X_train_1000, y_train_1000)\n",
    "\n",
    "# Reinitialize and train the Decision Tree classifier\n",
    "dt_classifier_1000 = DecisionTreeClassifier()\n",
    "dt_classifier_1000.fit(X_train_1000, y_train_1000)\n",
    "\n",
    "# Predict and evaluate the models\n",
    "y_pred_nb_1000 = nb_classifier_1000.predict(X_test_1000)\n",
    "y_pred_svm_1000 = svm_classifier_1000.predict(X_test_1000)\n",
    "y_pred_dt_1000 = dt_classifier_1000.predict(X_test_1000)\n",
    "\n",
    "# Store the evaluation metrics for the new models\n",
    "model_metrics_1000 = {\n",
    "    \"Naive Bayes (1000 features)\": {\n",
    "        \"Accuracy\": nb_classifier_1000.score(X_test_1000, y_test_1000),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_1000, y_pred_nb_1000)\n",
    "    },\n",
    "    \"SVM (1000 features)\": {\n",
    "        \"Accuracy\": svm_classifier_1000.score(X_test_1000, y_test_1000),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_1000, y_pred_svm_1000)\n",
    "    },\n",
    "    \"Decision Tree (1000 features)\": {\n",
    "        \"Accuracy\": dt_classifier_1000.score(X_test_1000, y_test_1000),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_1000, y_pred_dt_1000)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the comparison with updated format\n",
    "print_model_comparison_percentage(model_metrics_1000)\n",
    "\n",
    "# print the shape of the new TF-IDF Vectorizer\n",
    "print(tfidf_1000.toarray().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes (5000 features)\n",
      "  Accuracy: 97.49%\n",
      "  Confusion Matrix:\n",
      "[[965   0]\n",
      " [ 28 122]]\n",
      "\n",
      "Model: SVM (5000 features)\n",
      "  Accuracy: 97.49%\n",
      "  Confusion Matrix:\n",
      "[[963   2]\n",
      " [ 26 124]]\n",
      "\n",
      "Model: Decision Tree (5000 features)\n",
      "  Accuracy: 96.32%\n",
      "  Confusion Matrix:\n",
      "[[947  18]\n",
      " [ 23 127]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the TF-IDF Vectorizer with max_features set to 5000\n",
    "tfidf_vectorizer_5000 = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the processed text\n",
    "tfidf_5000 = tfidf_vectorizer_5000.fit_transform(norm_spam)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_5000, X_test_5000, y_train_5000, y_test_5000 = train_test_split(tfidf_5000, spam['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Reinitialize and train the Naive Bayes classifier\n",
    "nb_classifier_5000 = MultinomialNB()\n",
    "nb_classifier_5000.fit(X_train_5000, y_train_5000)\n",
    "\n",
    "# Reinitialize and train the SVM classifier\n",
    "svm_classifier_5000 = SVC()\n",
    "svm_classifier_5000.fit(X_train_5000, y_train_5000)\n",
    "\n",
    "# Reinitialize and train the Decision Tree classifier\n",
    "dt_classifier_5000 = DecisionTreeClassifier()\n",
    "dt_classifier_5000.fit(X_train_5000, y_train_5000)\n",
    "\n",
    "# Predict and evaluate the models\n",
    "y_pred_nb_5000 = nb_classifier_5000.predict(X_test_5000)\n",
    "y_pred_svm_5000 = svm_classifier_5000.predict(X_test_5000)\n",
    "y_pred_dt_5000 = dt_classifier_5000.predict(X_test_5000)\n",
    "\n",
    "# Store the evaluation metrics for the new models\n",
    "model_metrics_5000 = {\n",
    "    \"Naive Bayes (5000 features)\": {\n",
    "        \"Accuracy\": nb_classifier_5000.score(X_test_5000, y_test_5000),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_5000, y_pred_nb_5000)\n",
    "    },\n",
    "    \"SVM (5000 features)\": {\n",
    "        \"Accuracy\": svm_classifier_5000.score(X_test_5000, y_test_5000),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_5000, y_pred_svm_5000)\n",
    "    },\n",
    "    \"Decision Tree (5000 features)\": {\n",
    "        \"Accuracy\": dt_classifier_5000.score(X_test_5000, y_test_5000),\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test_5000, y_pred_dt_5000)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the comparison with updated format\n",
    "print_model_comparison_percentage(model_metrics_5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparación accuracy de los modelos sin limite, con 1000 y 5000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sin Límite</th>\n",
       "      <th>1000 Features</th>\n",
       "      <th>5000 Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>96.68</td>\n",
       "      <td>97.94</td>\n",
       "      <td>97.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>97.22</td>\n",
       "      <td>97.94</td>\n",
       "      <td>97.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>96.95</td>\n",
       "      <td>96.41</td>\n",
       "      <td>96.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Sin Límite  1000 Features  5000 Features\n",
       "Naive Bayes         96.68          97.94          97.49\n",
       "SVM                 97.22          97.94          97.49\n",
       "Decision Tree       96.95          96.41          96.41"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores de precisión previamente calculados y discutidos\n",
    "accuracy_values = {\n",
    "    \"Naive Bayes\": {\"Sin Límite\": 96.68, \"1000 Features\": 97.94, \"5000 Features\": 97.49},\n",
    "    \"SVM\": {\"Sin Límite\": 97.22, \"1000 Features\": 97.94, \"5000 Features\": 97.49},\n",
    "    \"Decision Tree\": {\"Sin Límite\": 96.95, \"1000 Features\": 96.41, \"5000 Features\": 96.41}\n",
    "}\n",
    "\n",
    "# Crear DataFrame para la comparación\n",
    "accuracy_df = pd.DataFrame(accuracy_values).T\n",
    "\n",
    "# Imprimir la tabla de comparación\n",
    "accuracy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Modifica el resultado si no se eliminan las stop words?\n",
    "Dado que la vectorización con 1000 características mostró una mejora general en el rendimiento para los modelos de Naive Bayes y SVM, y un rendimiento estable para Decision Tree, me centraré en este escenario. Además, 1000 características representan un buen equilibrio entre la retención de información importante y la reducción de la dimensionalidad, lo cual es crucial para entender el impacto de las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizar el texto quitando signos de puntuacion y dejando las stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "wpt = WordPunctTokenizer()\n",
    "\n",
    "def normalize_document_stop(doc):\n",
    "    doc = doc.replace(\"!\", \"\").replace(\"¡\", \"\").replace(\",\", \"\").replace(\".\", \"\").replace(\";\", \"\").replace(\"'\", \"\").replace('\"', \"\").replace(\"?\", \"\").replace(\"¿\", \"\").replace(\":\", \"\")\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # re-create document\n",
    "    doc = ' '.join(tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question ( std txt rate ) t & cs apply 08452810075over18s',\n",
       " 'u dun say so early hor u c already then say',\n",
       " 'nah i dont think he goes to usf he lives around here though',\n",
       " 'freemsg hey there darling its been 3 weeks now and no word back id like some fun you up for it still tb ok xxx std chgs to send å £ 150 to rcv',\n",
       " 'even my brother is not like to speak with me they treat me like aids patent',\n",
       " 'as per your request melle melle ( oru minnaminunginte nurungu vettam ) has been set as your callertune for all callers press * 9 to copy your friends callertune',\n",
       " 'winner as a valued network customer you have been selected to receivea å £ 900 prize reward to claim call 09061701461 claim code kl341 valid 12 hours only',\n",
       " 'had your mobile 11 months or more u r entitled to update to the latest colour mobiles with camera for free call the mobile update co free on 08002986030',\n",
       " 'im gonna be home soon and i dont want to talk about this stuff anymore tonight k ive cried enough today',\n",
       " 'six chances to win cash from 100 to 20000 pounds txt > csh11 and send to 87575 cost 150p / day 6days 16 + tsandcs apply reply hl 4 info',\n",
       " 'urgent you have won a 1 week free membership in our å £ 100000 prize jackpot txt the word claim to no 81010 t & c wwwdbuknet lccltd pobox 4403ldnw1a7rw18',\n",
       " 'ive been searching for the right words to thank you for this breather i promise i wont take your help for granted and will fulfil my promise you have been wonderful and a blessing at all times',\n",
       " 'i have a date on sunday with will',\n",
       " 'xxxmobilemovieclub to use your credit click the wap link in the next txt message or click here >> http // wap xxxmobilemovieclubcomn = qjkgighjjgcbl',\n",
       " 'oh kim watching here )',\n",
       " 'eh u remember how 2 spell his name yes i did he v naughty make until i v wet',\n",
       " 'fine if thatåõs the way u feel thatåõs the way its gota b',\n",
       " 'england v macedonia - dont miss the goals / team news txt ur national team to 87077 eg england to 87077 trywales scotland 4txt / ì¼120 poboxox36504w45wq 16 +']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_spam_stop=[]\n",
    "\n",
    "for document in spam['text']:\n",
    "    norm_spam_stop.append(normalize_document_stop(document))\n",
    "\n",
    "#ver solo los primeros 20 documentos normalizados\n",
    "norm_spam_stop[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_stop = tfidf_vectorizer.fit_transform(norm_spam_stop)\n",
    "\n",
    "tfidf_array_stop = tfidf_stop.toarray()\n",
    "tfidf_array_stop.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividir el dataset en train (80%) y test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_stop, spam['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skjdbv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 96.68%\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[965   0]\n",
      " [ 37 113]]\n",
      "\n",
      "SVM Accuracy: 97.22%\n",
      "SVM Confusion Matrix:\n",
      "[[963   2]\n",
      " [ 29 121]]\n",
      "\n",
      "Decision Tree Accuracy: 96.95%\n",
      "Decision Tree Confusion Matrix:\n",
      "[[951  14]\n",
      " [ 20 130]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# entenar modelo bayesiano ingenuo, svm y arbol de decision\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "svm = SVC()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#predecir con el modelo entrenado\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "#evaluar el modelo con accuracy\n",
    "nb_acc_stop = accuracy_score(y_test, y_pred_nb)\n",
    "svm_acc_stop = accuracy_score(y_test, y_pred_svm)\n",
    "dt_acc_stop = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "#evaluar el modelo con la matriz de confusion\n",
    "nb_conf_stop = confusion_matrix(y_test, y_pred_nb)\n",
    "svm_conf_stop = confusion_matrix(y_test, y_pred_svm)\n",
    "dt_conf_stop = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "# ver los resultados de accuracy con tres decimales y en porcentaje y la matriz de confusion\n",
    "print(f\"Naive Bayes Accuracy: {nb_acc*100:.2f}%\")\n",
    "print(f\"Naive Bayes Confusion Matrix:\\n{nb_conf}\\n\")\n",
    "print(f\"SVM Accuracy: {svm_acc*100:.2f}%\")\n",
    "print(f\"SVM Confusion Matrix:\\n{svm_conf}\\n\")\n",
    "print(f\"Decision Tree Accuracy: {dt_acc*100:.2f}%\")\n",
    "print(f\"Decision Tree Confusion Matrix:\\n{dt_conf}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el resultado es el mismo que entrenando los modelos quitando las stopwords pero sin limite de features. Es decir que la mejor combinación según el accuracy de los modelos sería quitar las stopwords y poner un limite de 1000 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Informe de Análisis de Clasificacion de Spam**\n",
    "\n",
    "Este informe presenta los resultados de un proyecto de clasificación de correos electrónicos en categorías de \"spam\" y \"no spam\" (ham). Se utilizó un conjunto de datos de correos electrónicos, y se aplicaron técnicas de Procesamiento de Lenguaje Natural (NLP) para transformar el texto en datos numéricos como es la matriz tfidf que permite cuantificar la importancia de palabras (o términos) en documentos.   \n",
    "El conjunto de datos consta de 5,572 correos electrónicos, etiquetados como 'spam' o 'ham'. Cada correo fue preprocesado mediante técnicas estándar de NLP, incluyendo la conversión a minúsculas, eliminación de caracteres especiales y puntuación, y tokenización para quitar despues las stopwords (palabras que a priori no añaden significado a los textos por su caracter común).\n",
    "  \n",
    "Se probaron tres modelos de aprendizaje automático: **Naive Bayes, Máquinas de Soporte Vectorial (SVM) y Árboles de Decisión**, con diferentes configuraciones de características para optimizar su rendimiento.   \n",
    "Se experimentó con tres configuraciones de vectorización TF-IDF:\n",
    "\n",
    "    - Sin límite de características\n",
    "    - Limitando a 1,000 características\n",
    "    - Limitando a 5,000 características\n",
    "Además, se consideró el impacto de la inclusión y exclusión de stopwords en la vectorización.\n",
    "\n",
    "##### </u>**Resultados**</u>\n",
    "Los modelos mostraron variaciones en su rendimiento según la configuración:\n",
    "\n",
    "**Naive Bayes**: Mejoró significativamente con la reducción a 1,000 características. La precisión pasó del 96.68% sin límite a 97.94% con 1,000 características. Con 5,000 características, la precisión fue ligeramente menor (97.49%) pero aun asi mejor que sin límite.\n",
    "\n",
    "**SVM**: Este modelo también mostró una mejora con 1,000 características, alcanzando una precisión del 97.94%. Con 5,000 características, la precisión fue un poco menor pero aún mejor que sin límite (97.49% frente a 97.22%).\n",
    "\n",
    "**Decision Tree**: Mostró un rendimiento más estable a través de diferentes configuraciones, aunque ligeramente mejor sin límite (96.95%).\n",
    "\n",
    "***Descubrimientos Clave***  \n",
    "\n",
    "La reducción de características mejora el rendimiento en Naive Bayes y SVM, probablemente al reducir el ruido y el overfitting. \n",
    "\n",
    "Los árboles de decisión son menos sensibles a la reducción de características, lo que sugiere una mayor robustez a diferentes representaciones de características. Es decir que no se ven tan afectados por la presencia o ausencia de ciertas características  \n",
    "\n",
    "La inclusión de stopwords podría ser relevante para evaluar, ya que su impacto puede variar según el modelo y el contexto del análisis. dicho esto, en las pruebas que he hecho, el unico modelo que se comportaba mejor con stopwords es el de decision trees, probablemente por lo mencionado en el punto anterior.\n",
    "\n",
    "Para futuras implementaciones, se recomienda utilizar el modelo SVM con 1,000 características y quitando stopwords para un equilibrio óptimo entre precisión y complejidad del modelo. Ademas, cabe considerar la experimentación con técnicas de ajuste de hiperparámetros y modelos más avanzados para posibles mejoras\n",
    "\n",
    "En conclusión, el proyecto demostró la viabilidad de clasificar eficazmente correos electrónicos como 'spam' o 'ham' utilizando técnicas de NLP y aprendizaje automático. Los modelos SVM y Naive Bayes mostraron un rendimiento particularmente prometedor, con mejoras significativas en la precisión al ajustar el número de características en la vectorización TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
